# coding:utf-8
"""
1. 时不时的会用到，需要保存大量的博客或论坛的文章，方便采集文章
2. 使用requests请求网页，BeautifulSoup解析网页，pdfkit保存html为pdf文件
3. 单线程太慢，使用map 并行请求
4. 程序是个小demo，对于不同的站点，需要更改程序：获得所有需要保存成pdf文件的网页的标题和URL的方法不同，以及pdf文名标准
5. 对于整个页面直接保存下来，主要是减少对不同博客、论坛网页的标签分析和提取的麻烦，也就是尽量简单，换个站点也适用
"""


from __future__ import unicode_literals
import sys
import pdfkit
import requests
from bs4 import BeautifulSoup
from multiprocessing.dummy import Pool as ThreadPool
try:
    from urlparse import urlparse
except:
    from urllib.parse import urlparse

reload(sys)
sys.setdefaultencoding('utf8')

class Spider(object):
    def __init__(self, _url):
        self.start_url = _url
        self.domain = '{uri.scheme}://{uri.netloc}'.format(uri=urlparse(self.start_url))

    def generate_url(self):
        headers = {"": ""}
        html_content = requests.get(self.start_url).content

        soup = BeautifulSoup(html_content, "html.parser")
        title = None

        _tags = soup.find_all("div")
        for _tag in _tags:
            if _tag.attrs["class"][0] == "archive-post":
                yield (title, self.domain + _tag.a["href"])

    @staticmethod
    def html2pdf(args):
        url, pdf_name = args[0], args[1]
        print("URL:{} save to PDF:{}".format(url, pdf_name))
        pdfkit.from_url(url, pdf_name, configuration=config)

    def run(self):
        arg = []
        for title, url in self.generate_url():
            title = url.rsplit("/")[-2]
            for x in str(title):
                if x in r'\/:*?"<>|':
                    title = str(title).replace(x, "-")
            pdf_name = str(title) + '.pdf'
            arg.append((url, pdf_name))

        pool.map(self.html2pdf, arg)
        pool.close()
        pool.join()

if __name__ == '__main__':
    path_wk = r'C:\Python\Lib\site-packages\wkhtmltopdf\bin\wkhtmltopdf.exe'

    pool = ThreadPool(4)
    config = pdfkit.configuration(wkhtmltopdf=path_wk)
    start_url = "https://klionsec.github.io/archives/"
    crawler = Spider(start_url)
    crawler.run()